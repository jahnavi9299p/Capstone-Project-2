{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "church\n",
      "aardwolf\n",
      "abacus\n",
      "hardrock\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#     WordNet Lemmatizer\n",
    "\n",
    "#     Lemmatize using WordNet's built-in morphy function.\n",
    "#     Returns the input word unchanged if it cannot be found in WordNet.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('dogs'))\n",
    "\n",
    "print(wnl.lemmatize('churches'))\n",
    "\n",
    "print(wnl.lemmatize('aardwolves'))\n",
    "\n",
    "print(wnl.lemmatize('abaci'))\n",
    "\n",
    "print(wnl.lemmatize('hardrock'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran\n",
      "run\n",
      "run\n",
      "running\n",
      "sprint\n"
     ]
    }
   ],
   "source": [
    "r = ['ran','runs','run','running','sprint']\n",
    "for i in r:\n",
    "    print(wnl.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('one.n.01'), Synset('one.n.02'), Synset('one.s.01'), Synset('one.s.02'), Synset('one.s.03'), Synset('one.s.04'), Synset('one.s.05'), Synset('one.s.06'), Synset('matchless.s.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"one\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('one.n.01')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.wordnet.Synset"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = str(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Synset('one.n.01')\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds = []\n",
    "for elem in syns :\n",
    "    for l in elem.lemmas():\n",
    "        wrds.append(l.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " '1',\n",
       " 'I',\n",
       " 'ace',\n",
       " 'single',\n",
       " 'unity',\n",
       " 'one',\n",
       " 'one',\n",
       " '1',\n",
       " 'i',\n",
       " 'ane',\n",
       " 'one',\n",
       " 'unitary',\n",
       " 'one',\n",
       " 'one',\n",
       " 'one',\n",
       " 'one',\n",
       " 'matchless',\n",
       " 'nonpareil',\n",
       " 'one',\n",
       " 'one_and_only',\n",
       " 'peerless',\n",
       " 'unmatched',\n",
       " 'unmatchable',\n",
       " 'unrivaled',\n",
       " 'unrivalled']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wrds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey i am yann how are you and how is it going  that is interesting i would love to hear more about it\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(posts):\n",
    "    posts = re.sub( r'\\|\\|\\|', r' ', posts ) \n",
    "    phrase = re.sub( r'http\\S+', r'', posts )\n",
    "    # dealing specific contractions\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"hasn\\'t\", \"has not\", phrase)\n",
    "    # dealing general contractions\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    posts = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    posts = re.sub( r'(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\@)|(\\_)|(\\-)|(\\+)|(\\*)|(\\/)|(\\#)|(\\&)|(\\$)|(\\{)|(\\})','', posts )\n",
    "    posts = posts.lower()\n",
    "    return posts\n",
    "\n",
    "\n",
    "test = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it.\"\n",
    "print(decontracted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Janu&Vaibh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Janu&Vaibh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'one 'ace 'single\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(\"\"\"'one' \n",
    " '1' \n",
    " 'I' \n",
    " 'ace' \n",
    " 'single'\"\"\" \n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = RepeatReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi!'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.replace('hiiiiii!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    #text = text.decode(\"utf8\")\n",
    "    # tokenize into words\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    # remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    # remove words less than three letters\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    # lower capitalization\n",
    "    tokens = [l.replace(word.lower()) for word in tokens]\n",
    "    #tokens - [l.replace(word) for word in tokens]\n",
    "    # lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = decontracted(\"\"\"'Good one  _____   https://www.youtube.com/watch?v=fHiGbolFFGw|||\n",
    "Of course, to which I say I know; that's my blessing and my curse.|||\n",
    "Does being absolutely positive that you and your best friend could be an amazing couple count? If so, than yes.   Or it's more I could be madly in love in case I reconciled my feelings (which at...|||\n",
    "No, I didn't; thank you for a link!|||\n",
    "So-called Ti-Si loop (and it can stem from any current topic/obsession) can be deadly. It's like when you're stuck in your own thoughts, and your mind just wanders in circles. Feels truly terrible. ...|||\n",
    "Have you noticed how peculiar vegetation can be? All you have to do is look down at the grass: dozens of different plant species there.    And now imagine that hundreds of years later (when/if soil...|||\n",
    "The Smiths – Never Had No One Ever|||\n",
    "I often find myself spotting faces on marble tiles/wood.|||\n",
    "This 5 year-old sentence is an incredibly accurate and beautiful description.|||\n",
    "I haven't visited this website in the last 3 years. So whoever reads this (and maybe even remembers me, which I highly doubt): hi.  700049  700057|||When you sit in your garden until 10:30 PM writing songs, and sing them (together with dozens of crickets) while playing your acoustic guitar.|||\n",
    "This is the most INTP-ish thread I've ever seen.|||\n",
    "I wouldn't be able to look at the painting for the entire life if I knew that I picked it over the human being.|||\n",
    "I was drawing a background for my animation on which I'm working right now - it should have been Mars.. But I felt obligated to make Mark Watneyx92s postcard from it :D  If you read the book...|||\n",
    "I started to make comics about turtle Gordon and unicorn Chimes - here you can see two first stories: https://www.tumblr.com/blog/-alexxxandra-|||\n",
    "INTJ Recently I started to post my comics about two friends - turtle Gordon and unicorn Chimes. Before that, I just posted stuff that interested me, but from now on I'll try to include only my works...|||\n",
    "Probably we could work together on a new model - I'm an expert in abrupt explosions of laughter upon various weird stuff. That happens because of peculiar sense of humor - so peculiar that not much...|||\n",
    "Hellooo Nah, you can touch it. Everyone thinks that it's scared or sad, but that's not true - in fact it has an absolutely neutral face. And this kitten actually really likes patting and hugs (only...|||Well.. kind of; As it was already mentioned, sometimes because of Ni it's hard to convey complex stuff which pops up in your head in whimsical compilations of shapes and pictures only with words....|||\n",
    "I think this kitten would be very appropriate here.  376562|||367034|||\n",
    "GOOD NIGHT everyone out there! Even if for someone there is morning right now - nights always supersede mornings.. And people say good night in order to meet next day :)|||\n",
    "Oh, that movie :) It's awesome Thank you! Hope you had good sleep in the air; anyway, I'm wishing you good night for the next night ahead! (hopefully it will be on land)  Good people deserve good...|||\n",
    "358882  358890|||\n",
    "Well, other people who may be wondering about an issue from the name of the topic will find your response helpful anyway :)|||\n",
    "This. Finally someone mentioned that :)|||\n",
    "I still see creatures/faces in a maze of various random patterns. It can be amusing sometimes.  It's a very handy skill when you're bored.|||\n",
    "Oh, I didn't know that.. What a pity.  Why not sacrifice whole supermarket, then? We can decide which Walmart will be the best (I think the biggest one would be great).|||\n",
    "yippy  Here you go  357002  He thinks that the fire is delicious. Should I sacrifice tofu? I don't like to waste food.|||I don't think that the creator of this thread cares what's going on here after 3 years :)|||\n",
    "Heh, I understand you :) With these same given languages)))|||\n",
    "Yessss, Adventure Time :D|||\n",
    "I get angry quite rarely, but when I do, it's safer for surrounding people to go somewhere else. It's impossible for me to hide or suppress anger; the only way to get rid of this feeling is to burst...|||\n",
    "I've never liked it  Anything fake is bad, actually.|||\n",
    "Hugs should be given only to chosen ones. Chosen. There are quite few of them, though.|||349890|||\n",
    "Yup, you're doing it right :)|||\n",
    "http://-alexxxandra-.tumblr.com/|||256818|||\n",
    "Of course it's not very comfortable.  But. Human race survived thankfully women's ability to give birth to other human beings. It worked for thousands of years. Why change it? Besides, there are...|||\n",
    "That happens. And it occurs because most often people use results of extremely precise and elaborate online tests as a basis of determining one's type.      Both visual and language arts (more...|||246386|||\n",
    "I study graphic design now, which I really enjoy. What is interesting about this field, is that the ability to generate ideas and solve problems is much more important than possession of a specific...|||\n",
    "Alexxxandra97 - DeviantArt|||236994|||http://www.youtube.com/watch?v=2Nkcn8m9M0M|||\n",
    "I am always ready to discipline (to intimidate, to be precise) my sibling's offender.|||\n",
    "World domination? Shooting people in the head? Why?  Oh, right, INTJs always must be characterised only with these words.     I want to show so badly my reaction to this: 221226|||218106|||ISTP?   http://www.youtube.com/watch?v=7ghqoYxmaUE\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good one course say know blessing curse absolutely positive best friend could amazing couple count yes could madly love case reconciled feeling thank link socaled tisi loop stem current topicobsesion deadly like stuck thought mind wanders circle feel truly terrible noticed peculiar vegetation look grass dozen different plant specie imagine hundred year later whenif soil smith never one ever often find spotting face marble tileswod yearold sentence incredibly accurate beautiful description visited website last year whoever read maybe even remembers highly doubt sit garden writing song sing together dozen cricket playing acoustic guitar intpish thread ever seen would able look painting entire life knew picked human drawing background animation working right mar felt obligated make mark watneyxs postcard read book started make comic turtle gordon unicorn chime see two first story intj recently started post comic two friend turtle gordon unicorn chime posted stuff interested try include work probably could work together new model expert abrupt explosion laughter upon various weird stuff happens peculiar sense humor peculiar much hello nah touch everyone think scared sad true fact absolutely neutral face kitten actually really like patting hug well kind already mentioned sometimes hard convey complex stuff pop head whimsical compilation shape picture word think kitten would appropriate good night everyone even someone morning right night always supersede morning people say good night order meet next day movie awesome thank hope good sleep air anyway wishing good night next night ahead hopefully land good people deserve good well people may wondering issue name topic find response helpful anyway finally someone mentioned still see creaturesfaces maze various random pattern amusing sometimes handy skill bored know pity sacrifice whole supermarket decide walmart best think biggest one would great yipy think fire delicious sacrifice tofu like waste food think creator thread care going year heh understand given language yes adventure time get angry quite rarely safer surrounding people somewhere else impossible hide suppress anger way get rid feeling burst never liked anything fake bad actually hug given chosen one chosen quite though yup right course comfortable human race survived thankfully woman ability give birth human being worked thousand year change besides happens occurs often people use result extremely precise elaborate online test basis determining one type visual language art study graphic design really enjoy interesting field ability generate idea solve problem much important possession specific alexandra deviantart always ready discipline intimidate precise sibling offender world domination shooting people head right intjs always must characterised word want show badly reaction istp'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Janu&Vaibh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pete corrode a large patty . Sam have a big mouth .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import randint\n",
    "import nltk.data\n",
    "\n",
    "# Load a text file if required\n",
    "text = \"Pete ate a large cake. Sam has a big mouth.\"\n",
    "output = \"\"\n",
    "\n",
    "# Load the pretrained neural net\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized = tokenizer.tokenize(text)\n",
    "\n",
    "# Get the list of words from the entire text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Identify the parts of speech\n",
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "for i in range(0,len(words)):\n",
    "    replacements = []\n",
    "\n",
    "    # Only replace nouns with nouns, vowels with vowels etc.\n",
    "    for syn in wordnet.synsets(words[i]):\n",
    "\n",
    "        # Do not attempt to replace proper nouns or determiners\n",
    "        if tagged[i][1] == 'NNP' or tagged[i][1] == 'DT':\n",
    "            break\n",
    "        \n",
    "        # The tokenizer returns strings like NNP, VBP etc\n",
    "        # but the wordnet synonyms has tags like .n.\n",
    "        # So we extract the first character from NNP ie n\n",
    "        # then we check if the dictionary word has a .n. or not \n",
    "        word_type = tagged[i][1][0].lower()\n",
    "        if syn.name().find(\".\"+word_type+\".\"):\n",
    "            # extract the word only\n",
    "            r = syn.name()[0:syn.name().find(\".\")]\n",
    "            replacements.append(r)\n",
    "\n",
    "    if len(replacements) > 0:\n",
    "        # Choose a random replacement\n",
    "        replacement = replacements[randint(0,len(replacements)-1)]\n",
    "        output = output + \" \" + replacement\n",
    "    else:\n",
    "        # If no replacement could be found, then just use the\n",
    "        # original word\n",
    "        output = output + \" \" + words[i]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.punkt.PunktSentenceTokenizer at 0x1cb4f719d30>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tokenize.punkt.PunktSentenceTokenizer"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nltk.data.load('tokenizers/punkt/english.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
